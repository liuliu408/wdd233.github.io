---
layout: post
title:  "深度学习面试问题集锦"
date:   2018-12-31 15:30:58 +0800
categories: jekyll update
---
# 写在前面
简单总结了一下目前面试官能考察的深度学习(机器学习)知识点出处：
1.吴恩达深度学习deeplearning.ai  
2.深度学习(花书)  
3.经典神经网络paper(很多重要的DL概念其实在达书中都讲过)  
3.统计学习方法  
4.西瓜书  
面试过几家小公司，无一例外的问到了bn，pooling等等,知识点零零散散， 于是自己想整理一套面试宝典  
搜集了多个仓库，整理总结了一部分问题和答案，仅供参考。 
## 机器学习模型评估
也称为性能度量(performance measure)。设计好了一个模型，如何去评价其泛化能力呢？<br>
对于给定样例集$D={(x_1,y_1),(x_2,y_2)..(x_m,y_m)}$,其中$y_i$是真实标记，$x_i$,
### **回归任务**
最常用的性能度量是MSE(mean squared error)

$E(f;D)=\frac {1}{m}\sum _{i=1}^m(f(x_i)-y_i)^2$

>在一些回归任务诸如人群密度估计(crowd counting)中，也采用MAE(Mean Absolute Error)作为评价指标。
### 分类任务的性能度量
#### 错误率(Error)与精度(accuracy)
**精度(acc)定义:**<br>
$acc(f:D)=\frac{1}{m}\sum _{i=0}^m\Pi(f(x_i)=y_i)$

**错误率(Error)定义**<br>
1-acc

### 常见的一些范数及应用场景，如L0，L1,L2,Frobenius范数

**理解范数：**平方范数$L^2$常用来衡量向量的大小，可以简单地通过$X^TX$来计算。直观上来讲，向量`x`的范数衡量从原点到点`x`的距离。
$$||x||_p=\left(\sum|x_i|^p\right)^\frac{1}{p}$$


范数在机器学习的应用场景：**正则化项**


**正则化问题的引出：**

机器学习中的一个核心问题是：**设计不仅在训练数据上表现好，而且能在新的输入(test集)上泛化好的算法。**正则化就是显式地设计来减少**测试误差**(可能会增大**训练误差**为代价)。
在机器学习中，经常会遇到过拟合的现象
有些正则化策略向目标函数增加额外项来对参数值进行软约束，如果我们细心选择，这些额外的约束和惩罚可以改善模型在测试集上的表现。有时候，这些约束和乘法被设计为编码特定类型的先验知识；其他时候，这些约束和乘法被设计为偏好简单模型，以便提高泛化能力(模型过于复杂，存在在训练集上过拟合的风险，相对简单的模型可能存在更强的泛化能力)。

许多正则化方法通过对目标函数$J(\theta;X,y)$添加一个参数惩罚$\alpha \Omega(\theta)$
当我们的训练算法最小化正则化后的木变函数$\hat J(\theta)$时，他会降低原始目标$J$关于训练误差并同时减小在某些衡量标准下的参数规模。选择不同的参数$\theta$的范数会产生偏好不同的解。

#### $L^2$参数正则化
最简单最常见的参数惩罚范数惩罚，即通常被称为**权重衰减(weight decay)**的$L^2$的参数惩罚。这个正则化策略通过向目标函数添加一个正则化项使权重更加接近原点。$L^2$也被称为岭回归
$$w=w-lr*(\alpha w + \nabla J(w))=(1-lr*\alpha )w-lr*\nabla J(w)$$
加入权重衰减后会引起学习规则的衰减，即在没不执行通常的梯度更新之前先收缩权重向量。<br>

$L^2$（**权重衰减**）正则化对最佳w值的影响,
只有在显著减小目标函数方向上的参数$w$会保留得相对较好，在无助于目标函数减小的方向(对应Hessian矩阵较小特征值)上改变参数不会显著地增加梯度。这种不重要方向对应分量会在训练过程中因正则化而衰减掉

#### $L^1$正则化
L^2权重衰减是权重衰减是最常见的形式，还有其他方法限制模型参数的规模，一个选择是使用$L^1$正则化。
$$\Omega(\theta)=\sum |w_i|$$
$$\hat J(w;X,y)=\alpha ||w||_1+J(w;X,y)$$
对应的梯度
$$\hat J(w)=\alpha sign(w)+\nabla_wJ(w)$$
其中$sign(w)$只是简单地取w各个元素的正负号
我们可以看到正则化对梯度的影响不再是线性地缩放到每个$w_i$
相比较$L^2$正则化，**$L^1$会产生更稀疏的解。此处稀疏性指的是最优值中的一些参数w为0**。$L^2$正则化不会使参数变得稀疏，而$L^1$正则化有可能通过足够大的$\alpha$实现稀疏。


由$L^1$正则化的稀疏性质已经被广泛地用于特征选择(feature selection)机制。
#### 为何L1和L2正则化可以防止过拟合
* L1&L2正则化会使模型偏好于更小的权值
* 更小的权值意味着**更低的模型复杂度**；添加L1&L2相当于为模型添加了某种先验，限制了参数分布，从而降低了模型复杂度
* 模型复杂度降低，意味着模型对于噪声与异常点的抗干扰能力增强，从而提高模型的泛化能力




### 参数初始化
一般使用服从标准正态分布(高斯分布)mean=0, std=1，或者均匀分布的随机值作为**权重**的初始化参数；使用0作为偏置的初始化参数
一些启发式方法会根据输入与输出的单元数来决定初始值的范围
* 随机正交觉着呢

### Dropout策略
Dropout通过**参数共享**提供了一种廉价的Bagging集成近似——Dropout策略相当于继承了包括所有从基础网络出去部分单元后形成的子网络
通常，隐藏层的采样概率为0.5,输入的采样概率为0.8,；超参数也可以采样，但其采样概率一般为1

### BatchNormalization批正则化

训练的本质是**学习数据的分布**,如果训练数据与测试数据的分布不同会降低模型的泛化能力。因此，应该在开始训练器时对所有的数据做归一化处理，
而在神经网络中，每个隐藏层参数不同，会使下一层的输入发生变化，从而导致每一批数据的分布也发生变化；导致网络在每次迭代中都需要你和不同的数据分布，增大了网络的训练难度与过拟合的风险。
BN方法会针对每一批数据，在网络的每一层激活之前做归一化处理，使输入的均值为0，标准差为1.目的是将数据限制在统一的分布下

#### 基本原理
具体来说(针对DNN，全连接层构成的网络)，针对每一层第k个神经元，计算**批数据**在第`k`个神经元的均值与方差，然后将归一化后的值作为该神经元的激活值
$$\hat x_k=\frac {x_k-E[x_k]}{\sqrt{Var[x_k]}}$$
>计算$E[x_k]$和$Var[x_k]$使用batch维度做平均，可以看成在batch上的白化操作

BN对**层间数据分布进行额外约束**，但是这样会降低模型的拟合能力，破坏了之前学到的**特征分布**,为了**恢复数据的原始分布**，BN引入了一个**重构变化**，也就是可学习的$\gamma$和$\beta$

训练的时候通过全局的batch计算出了期望和方差，但是在测试的时候，通常测试数据只有一个，怎么办呢？

使用**全局统统计量**来替代批统计量
训练每个batch时，都会得到一组`(均值，方差)`
根据概率论学过的使用**样本无偏估计**，即在训练的时候就要计算这两个参数用于推断(BN层里面藏了不少东西^_^)
$E[x]=E[\mu_i]$<br>
$Var[x]=\frac{m}{m-1}E[\sigma_i^2]$


batch Normalizaiton作为近一年来DL的重要成果，已经被广泛证明其有效性和重要性。虽然有些细节处理还解释不清
机器学习领域有一个很重要的假设：IID(独立同分布假设)，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保证。Batch Normalization就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同的分布

##### 随着网络深度加深，训练起来困难，收敛越来越慢？
这个是DL领域很接近本质的好问题。很多论文都是来解决这个问题的，例如RelU和ResNet
对于深度学习这种包含很多隐藏层网络结构，在训练过程中，因为各层参数在不停地变化，所以每个隐藏层都会面临covariate shift(ICS)问题，也就是在训练过程中，隐藏层输入分布老是变来边去，提出了BatchNorm基本思想：能不能让每个隐藏层的节点的激活输入分布固定下来了？BN出现了..


为什么随着网络深度加深，训练起来越来越困难，收敛越来越慢？从论文的
mini-sgd对于One exmaple的两个优势：梯度更新方向更准确；并行计算速度更快；
covariate shift：如果ML系统实例集合<X,Y>中的输入值X的分布老师变，这不符合IID假设。对于深度学习这种包含很多隐藏层的网络结构，在训练过程中，因为各层参数在不停变化，所以每个因曾都会面临covariate shift问题，也就是在训练过程中，隐藏层的输入分布老是变来边去

### BN的推理(Inference)过程
**思考：**BN在训练的时候可以根据Mini-Batch里的若干训练实例进行激活函数调整，但是在推理过程中，很明显输入只有一个实例，看不到Mini-Batch其他实例，无法求均值和方差那么这时候如何对输入做BN呢？
既然没有从Mini-Batch数据中可以得到的统计量，那就想其他办法来获得均值方差。可以从所有的训练实例中获得统计量来代替Mini-Batch里面的m个训练实例

### BN的优缺点再次总结
**优点：**<br>
1.加速网络学习收敛(消除了ICS，支持更大的学习率)，提高模型准确率
2.防止了过拟合
3.降低了对参数初始化的需求
**缺点：**
1. 十分依赖Batch size，一定要batch size足够大，效果才好；
2. 不适合用于序列数据神经网络RNN，

### 其他的归一化方式介绍
layer Norm是在batch上面，对NHW做归一化，对小batchsize效果不好
InstanceNorm实在图像像素上，对HW做归一化，用在风格迁移
GroupNorm将channel分布，然后再做归一化
SwitchableNorm是将BN,LN,IN结合，赋给权重，让网络自己学习归一化应该用什么方法



### 简单介绍一下贝叶斯概率与频率派概率，以及在统计中对于真实参数的假设
**答：**
直接与时间发生的频率相联系，被称为频率派概率;而后者涉及确定性水平，被称为贝叶斯概率
关于不确定性的常识推理，如果我们已经列出了若干条期望它具有的性质，那么满足这些性质的唯一方法就是将贝叶斯概率和频率派概率视为等同

### 介绍一下sigmoid, relu, tanh, RBF及应用场景
**答：**

#### sigmoid 
$$g(z)=\frac{1}{1+e^(-z)}$$
特点：求导方便；值域(0,1)；头尾有饱和效应，对输入微小变化不敏感;在大部分定义域内饱和
$$\frac{d}{dz}g(z)=\frac{1}{1+e^{-z}}(1-\frac{1}{1+e^{-z}})=g(z)(1-g(z))$$

#### tanh
$$g(z)=tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}$$ 

#### ReLU？？缓解梯度爆炸？
ReLU通常是比较好的激活函数
**ReLU相比sigmoid的优势**
1.缓解了sigmoid的梯度饱和区域，缓解了梯度消失
2.计算起来更加方便，加速计算，在负半区域梯度为0，正半区域为1(0处导数为0，人为定义了导数保证了梯度的可学习性)
#### softmax
很自然地表示了具有k个可能值的离散型随机变量的概率分布

#### softplus
$$g(z)=log(1+exp(z))$$
通过图像可以看到是ReLU的平滑版本，但从经验来看，并没有
#### 为什么要使用非线性激活函数？
使用**激活函数**的目的是为了向网络中加入**非线性因素**，从而加强网络的表示能力，解决**线性模型**无法解决的问题
* 如果不适用非线性函数，整个网络都是一个线性组合函数
#### 为什么加入非线性因素能够加强网络的表示能力？——神经网络的万能近似定力
* 神经网络的万能近似定力认为给予网络足够数量的隐藏单元，他就能可以以任意的精度来映射**从一个有限空间到另一个有限空间的函数**

## 损失函数
交叉熵是非负的，在神经元达到很好的正确率的时候会接近0。这些其实就是我们想要的代价函数的特性。其实这些特性也是二次代价函数具备的。所以，交叉熵就是很好的选择了。交叉熵代价函数有一个比二次代价函数更好的特性就是它避免了学习速度下降的问题

$$\frac{\partial C}{\partial w_j}=-\frac{1}{n}\sum (s)$$

#### 感受野(Receptive filed)
在卷积神经网络CNN中，决定某一层输出结果中一个元素所对应的输入层的区域大小，被称作感受野

### 选择超参数
手动选择和自动选择。自动选择往往需要更高的计算成本
学习率可能是最重要的超参数。如果你只有时间调整一个超参数，那就是调整学习率！！
当学习率适合优化问题是，模型的有效容量最高，此时学习率是正确的，学习率关于训练误差具有U型曲线，，当学习率过大时，梯度下降可能会不经意地增加而非减少训练误差。当学习率太小时，训练不仅慢，还有可能永久听列在一个很高的训练误差上。

#### 自动超参数优化算法
1. 网格搜索
通常当超参数量较小的时候，可以使用网格搜索法。即列出每个超参数的大致候选集合。利用这些集合进行逐项组合优化。在条件允许的情况下，重复进行网格搜索会相当优秀，当然每次重复需要根据上一步得到的最优参数组合，进行进一步的细粒度调整。网格搜索最大的问题在于计算时间会随超参数的数量指数级增长。

2. 随机搜索
不需要设定一个离散的超参数集合，而是对每个超参数定义一个分布函数来生成随机超参数。例如在batch size=[16,32,64]中随机搜索

### softmax求导
当i=j:<br>
$S_i(1-S_i)$

当i$\ne$j:<br>


大规模分布式实现
模型并行
多个机器共同运行同一个数据点，每个机器负责模型的一部分。

数据并行

## 随机梯度下降(SGD)
在机器学习优化算法中，GD（gradient descent）是最常用的方法之一，简单来说就是在整个训练集中计算当前的梯度，选定一个步长进行更新。GD的优点是，基于整个数据集得到的梯度，梯度估计相对较准，更新过程更准确。但也有几个缺点，一个是当训练集较大时，GD的梯度计算较为耗时，二是现代深度学习网络的loss function往往是非凸的，基于凸优化理论的优化算法只能收敛到local minima，因此使用GD训练深度神经网络，最终收收敛点很容易落在初始点附近的一个local minima，不太容易达到较好的收敛性能。折中的方案就是mini-batch，一次采用batch size的sample来估计梯度，这样梯度估计相对于SGD更准，同时batch size能占满CPU/GPU的计算资源，又不像GD那样计算整个训练集。同时也由于mini batch能有适当的梯度噪声[8]，一定程度上缓解GD直接掉进了初始点附近的local minima导致收敛不好的缺点，所以mini-batch的方法也最为常用。

**我们平时提到的SGD通常就是用mini-batch梯度下降**，mini-batch来确定当次
通过计算少量样本的平均值可以快速得到一个对于实际梯度$\nabla C_x$，来估算梯度，加速学习过程,当然mini-batch也可以是在总的数据集中也可以使随机的。

小批量的大小通常由以下几个因素决定：
* 更大的批量会计算更精确地梯度估计，但也不是越大越好，超过一定的范围会降低
极小批量通常难以充分利用多核并行计算架构(如果计算量太小，即使多卡也没有什么太好的加速效果，因为单卡足以胜任工作)
* 如果批量处理中的所有样本可以并行地处理，那么内存消耗和批量大小会成正比。硬件可能是batch size的限制因素,因此batch size是一个重要的调参对象

通常是在犯错比较明显的时候学习的速度最快



## Dropout
Dropout可以被认为是集成大量深层神经网络的实用Bagging方法。Bagging涉及训练多个模型，每个测试样本上评估多个模型，Dropout提供了一种廉价版的Bagging集成。在dropout情况下，大部分模型共享参数

* 在 Bagging 的情况下，所有模型都是独立的；而在 Dropout 的情况下，所有模型共享参数，其中每个模型继承父神经网络参数的不同子集。
* 在 Bagging 的情况下，每一个模型都会在其相应训练集上训练到收敛。而在 Dropout 的情况下，通常大部分模型都没有显式地被训练；取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。


## 参数初始化
* 一般使用服从的高斯分布`（mean=0, stddev=1）`或均匀分布的随机值作为权重的初始化参数；使用 0 作为偏置的初始化参数
* 一些启发式方法会根据输入与输出的单元数来决定初始值的范围 
* 其他初始化方法
    随机正交矩阵（Orthogonal）
    截断高斯分布（Truncated normal distribution）


## 为什么要归一化？？

 为了后面数据处理的方便，归一化的确可以避免一些不必要的数值问题。
    为了程序运行时收敛加快。 下面图解。
    同一量纲。样本数据的评价标准不一样，需要对其量纲化，统一评价标准。这算是应用层面的需求。
    避免神经元饱和。啥意思？就是当神经元的激活在接近 0 或者 1 时会饱和，在这些区域，梯度几乎为 0，这样，在反向传播过程中，局部梯度就会接近 0，这会有效地“杀死”梯度。
    保证输出数据中数值小的不被吞食。
    
上图是代表数据是否均一化的最优解寻解过程（圆圈可以理解为等高线）。左图表示未经归一化操作的寻解过程，右图表示经过归一化后的寻解过程。

当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛；而右图对两个原始特征进行了归一化，其对应的等高线显得很圆，在梯度下降进行求解时能较快的收敛。

因此如果机器学习模型使用梯度下降法求最优解时，归一化往往非常有必要，否则很难收敛甚至不能收敛。

## 卷积网络CNN
### 卷积的动机
卷积三大重要思想：
* 稀疏交互(Sparse interactions)
* 参数共享(parameter sharing)
* 等变表示(equivariant representations)

传统的神经网络中每一个输出单元与每一个输入单元都产生交互。卷积网络改进了这一点，使具有稀疏交互的特征。CNN通过核(kernel)尺寸小鱼输入的尺寸来达到这个目的
### 稀疏带来的好处
* 提高了模型的效率
* 减少了模型的存储需求和计算量，如果有`m`个输入和`n`个输出，复杂度为`o(m×n)`，如果每一个输出只来自`k`个输入，那么计算复杂度为`o(k×n)`
虽然减少了隐藏层单元之间的交互，但是实际上在**深层的单元可以间接的连接到全部或者大部分的输入(这也就是所谓的感受野)**
### 参数共享
参数共享是指在一个模型中的多个函数中使用相同的参数，也就是说一对输入和输出只绑定一组参数，在不同的输入节点中滑动
### 等变/不变性
平移不变性是一个很有用的性质，尤其是但我们关心某个特征是否出现而不太关心其出现的具体位置时。
参数共享和(池化)使神经网络具有一定的平移不变性
>池化操作也能够加强网络的平移不变性
### 卷积参数计算
#### 输出feature map尺寸
$$\frac{n-f+p}{s} +1$$
n:输入尺寸 f:卷积核ch存 s:卷积步长 p:padding数量 
>只有卷积核完全覆盖输入计算才有效，但上式算出来的商可能不是整数，惯例是向下取整
#### 卷积conv2d以及参数量
通常二维图像有多个通道，因此每一张图像实际上的shape是(channel,h,w)，conv2d做的其实是三维卷积，conv2d不仅仅会对二维图像输出后的尺寸变动，更重要的是在通道上的变动，在通道上多样的处理方法也衍生出了很多经典网络

**conv2d权重参数量:**<br>
$$out×in×k×k$$
out:输出通道数量 in:输入通道数量 k:卷积核尺寸
#### 卷积运算计算量
计算conv2d在对输入的feature map卷积的计算量,总体思路是`输出尺寸×卷积的参数量`
$$(\frac{n-f+p}{s}+1)\times out \times in \times k \times k$$
>不考虑求和和bias计算量
### 卷积中padding

1. valid卷积——不使用0填充，卷积核只允许访问图像中能够完全包含整个核的位置，输出的宽度为`n(输入图像尺寸)-f(卷积核的尺寸)+1`(假设stride=1)
在这种情况下，输出的大小每一次都会损失一部分，(一般情况下，影响不大，除非是上百层的网络)
2. same卷积——进行足够的0填充保持每一次输出和输入具有相同大小的尺寸。
$p=(k-1)/2$
p：padding填充数量  k:卷积核的尺寸


<table style="width:100%; table-layout:fixed;">
  <tr>
    <td><img width="150px" src="/gif/no_padding_no_strides.gif"></td>
    <td><img width="150px" src="/gif/arbitrary_padding_no_strides.gif"></td>
    <td><img width="150px" src="/gif/same_padding_no_strides.gif"></td>
    <td><img width="150px" src="/gif/full_padding_no_strides.gif"></td>
  </tr>
  <tr>
    <td>无填充，步长为1</td>
    <td>Arbitrary padding, no strides</td>
    <td>same 模式，有填充，步长为1</td>
    <td>Full padding, no strides</td>
  </tr>
  <tr>
    <td><img width="150px" src="/gif/no_padding_strides.gif"></td>
    <td><img width="150px" src="/gif/padding_strides.gif"></td>
    <td><img width="150px" src="/gif/padding_strides_odd.gif"></td>
    <td></td>
  </tr>
  <tr>
    <td>无填充，步长为2</td>
    <td>Padding, strides</td>
    <td>Padding, strides (odd)</td>
    <td></td>
  </tr>
</table>

## 反卷积(Transposed convolution) 

蓝色是输入的feature map, 灰绿色是输出

<table style="width:100%; table-layout:fixed;">
  <tr>
    <td><img width="150px" src="/gif/no_padding_no_strides_transposed.gif"></td>
    <td><img width="150px" src="/gif/arbitrary_padding_no_strides_transposed.gif"></td>
    <td><img width="150px" src="/gif/same_padding_no_strides_transposed.gif"></td>
    <td><img width="150px" src="/gif/full_padding_no_strides_transposed.gif"></td>
  </tr>
  <tr>
    <td>No padding, no strides, transposed</td>
    <td>Arbitrary padding, no strides, transposed</td>
    <td>Half padding, no strides, transposed</td>
    <td>Full padding, no strides, transposed</td>
  </tr>
  <tr>
    <td><img width="150px" src="/gif/no_padding_strides_transposed.gif"></td>
    <td><img width="150px" src="/gif/padding_strides_transposed.gif"></td>
    <td><img width="150px" src="/gif/padding_strides_odd_transposed.gif"></td>
    <td></td>
  </tr>
  <tr>
    <td>No padding, strides, transposed</td>
    <td>Padding, strides, transposed</td>
    <td>Padding, strides, transposed (odd)</td>
    <td></td>
  </tr>
</table>

## 空洞卷积 (Dilated convolution) 
蓝色是输入的feature map, 灰绿色是输出

<table style="width:25%"; table-layout:fixed;>
  <tr>
    <td><img width="150px" src="/gif/dilation.gif"></td>
  </tr>
  <tr>
    <td>无填充，步长为1</td>
  </tr>
</table>
